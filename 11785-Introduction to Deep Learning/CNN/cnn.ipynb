{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "import numpy as np\n",
    "\n",
    "class CNN_B():\n",
    "    def __init__(self):\n",
    "        # Your initialization code goes here\n",
    "        self.layers = []\n",
    "        self.layers.append(Conv1D(24,8,8,4))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Conv1D(8,16,1,1))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Conv1D(16,4,1,1))\n",
    "        self.layers.append(Flatten())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def init_weights(self, weights):\n",
    "        # Load the weights for your CNN from the MLP Weights given\n",
    "        \n",
    "        \n",
    "        self.layers[0].W = weights[0].reshape(self.layers[0].W.shape[2],self.layers[0].W.shape[1],self.layers[0].W.shape[0]).transpose()\n",
    "        self.layers[2].W = weights[1].reshape(self.layers[2].W.shape[2],self.layers[2].W.shape[1],self.layers[2].W.shape[0]).transpose()\n",
    "        self.layers[4].W = weights[2].reshape(self.layers[4].W.shape[2],self.layers[4].W.shape[1],self.layers[4].W.shape[0]).transpose()\n",
    "        \n",
    "        \n",
    "  \n",
    "        \n",
    "#         raise NotImplemented\n",
    "\n",
    "    def forward(self, x):\n",
    "        # You do not need to modify this method\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def backward(self, delta):\n",
    "        # You do not need to modify this method\n",
    "        for layer in self.layers[::-1]:\n",
    "            delta = layer.backward(delta)\n",
    "        return delta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CNN_C():\n",
    "    def __init__(self):\n",
    "        # Your initialization code goes here\n",
    "        self.layers = []\n",
    "        self.layers.append(Conv1D(24,2,2,2))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Conv1D(2,8,2,2))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Conv1D(8,4,2,1))\n",
    "        self.layers.append(Flatten())\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def init_weights(self, weights):\n",
    "        # Load the weights for your CNN from the MLP Weights given\n",
    "#         print(weights[0].shape)\n",
    "#         print(weights[1].shape)\n",
    "#         print(weights[2].shape)\n",
    "        \n",
    "#         print(self.layers[0].W.shape)\n",
    "#         print(self.layers[2].W.shape)\n",
    "#         print(self.layers[4].W.shape)\n",
    "        \n",
    "        weights[0] = weights[0][:48,:2]\n",
    "        weights[1] = weights[1][:4,:8]\n",
    "        \n",
    "        self.layers[0].W = weights[0].reshape(self.layers[0].W.shape[2],self.layers[0].W.shape[1],self.layers[0].W.shape[0]).transpose()\n",
    "        self.layers[2].W = weights[1].reshape(self.layers[2].W.shape[2],self.layers[2].W.shape[1],self.layers[2].W.shape[0]).transpose()\n",
    "        self.layers[4].W = weights[2].reshape(self.layers[4].W.shape[2],self.layers[4].W.shape[1],self.layers[4].W.shape[0]).transpose()\n",
    "\n",
    "#         print(self.layers[0].W)\n",
    "#         print(self.layers[2].W)\n",
    "#         print(self.layers[4].W)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # You do not need to modify this method\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "#             print(\"layer\")\n",
    "        return out\n",
    "\n",
    "    def backward(self, delta):\n",
    "        # You do not need to modify this method\n",
    "        for layer in self.layers[::-1]:\n",
    "            delta = layer.backward(delta)\n",
    "        return delta\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_B():\n",
    "    def __init__(self):\n",
    "        # Your initialization code goes here\n",
    "        self.layers = []\n",
    "        self.layers.append(Conv1D(24,8,8,4))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Conv1D(8,16,1,1))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Conv1D(16,4,1,1))\n",
    "        self.layers.append(Flatten())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def init_weights(self, weights):\n",
    "        # Load the weights for your CNN from the MLP Weights given\n",
    "\n",
    "#         a = 0\n",
    "        \n",
    "        print(self.layers[0].W.shape)\n",
    "        print(self.layers[2].W.shape)\n",
    "        print(self.layers[4].W.shape)\n",
    "        \n",
    "#         print(weights[0].shape)\n",
    "#         print(weights[1].shape)\n",
    "#         print(weights[2].shape)\n",
    "        \n",
    "        \n",
    "        self.layers[0].W = weights[0].reshape(self.layers[0].W.shape[2],self.layers[0].W.shape[1],self.layers[0].W.shape[0]).transpose()\n",
    "        self.layers[2].W = weights[1].reshape(self.layers[2].W.shape[2],self.layers[2].W.shape[1],self.layers[2].W.shape[0]).transpose()\n",
    "        self.layers[4].W = weights[2].reshape(self.layers[4].W.shape[2],self.layers[4].W.shape[1],self.layers[4].W.shape[0]).transpose()\n",
    "        \n",
    "        \n",
    "  \n",
    "        \n",
    "#         raise NotImplemented\n",
    "\n",
    "    def forward(self, x):\n",
    "        # You do not need to modify this method\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def backward(self, delta):\n",
    "        # You do not need to modify this method\n",
    "        for layer in self.layers[::-1]:\n",
    "            delta = layer.backward(delta)\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_C():\n",
    "    def __init__(self):\n",
    "        # Your initialization code goes here\n",
    "        self.layers = []\n",
    "        self.layers.append(Conv1D(24,2,2,2))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Conv1D(2,8,2,2))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Conv1D(8,4,2,1))\n",
    "        self.layers.append(Flatten())\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def init_weights(self, weights):\n",
    "        # Load the weights for your CNN from the MLP Weights given\n",
    "#         print(weights[0].shape)\n",
    "#         print(weights[1].shape)\n",
    "#         print(weights[2].shape)\n",
    "        \n",
    "#         print(self.layers[0].W.shape)\n",
    "#         print(self.layers[2].W.shape)\n",
    "#         print(self.layers[4].W.shape)\n",
    "        \n",
    "        weights[0] = weights[0][:48,:2]\n",
    "        weights[1] = weights[1][:4,:8]\n",
    "        \n",
    "        self.layers[0].W = weights[0].reshape(self.layers[0].W.shape[2],self.layers[0].W.shape[1],self.layers[0].W.shape[0]).transpose()\n",
    "        self.layers[2].W = weights[1].reshape(self.layers[2].W.shape[2],self.layers[2].W.shape[1],self.layers[2].W.shape[0]).transpose()\n",
    "        self.layers[4].W = weights[2].reshape(self.layers[4].W.shape[2],self.layers[4].W.shape[1],self.layers[4].W.shape[0]).transpose()\n",
    "\n",
    "#         print(self.layers[0].W)\n",
    "#         print(self.layers[2].W)\n",
    "#         print(self.layers[4].W)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # You do not need to modify this method\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "#             print(\"layer\")\n",
    "        return out\n",
    "\n",
    "    def backward(self, delta):\n",
    "        # You do not need to modify this method\n",
    "        for layer in self.layers[::-1]:\n",
    "            delta = layer.backward(delta)\n",
    "        return delta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_part_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 8)\n",
      "(8, 16)\n",
      "(16, 4)\n",
      "(2, 24, 2)\n",
      "(8, 2, 2)\n",
      "(4, 8, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_part_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import traceback\n",
    "# def test_part_b():\n",
    "#     data = np.loadtxt('data/data.asc').T.reshape(1, 24, -1)\n",
    "# #     print(data.shape)\n",
    "#     cnn = CNN_B()\n",
    "#     weights = np.load('weights/mlp_weights_part_b.npy',allow_pickle=True)\n",
    "#     cnn.init_weights(weights)\n",
    "\n",
    "#     expected_result = np.load('autograde/res_b.npy',allow_pickle=True)\n",
    "#     result = cnn(data)\n",
    "\n",
    "#     try:\n",
    "#         assert(type(result)==type(expected_result))\n",
    "# #         print(expected_result.shape)\n",
    "#         assert(result.shape==expected_result.shape)\n",
    "#         assert(np.allclose(result,expected_result))\n",
    "\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         traceback.print_exc()\n",
    "#         return False\n",
    "\n",
    "\n",
    "# def test_part_c():\n",
    "#     data = np.loadtxt('data/data.asc').T.reshape(1, 24, -1)\n",
    "#     cnn = CNN_C()\n",
    "#     weights = np.load('weights/mlp_weights_part_c.npy',allow_pickle=True)\n",
    "#     cnn.init_weights(weights)\n",
    "\n",
    "#     expected_result = np.load('autograde/res_c.npy',allow_pickle=True)\n",
    "#     result = cnn(data)\n",
    "\n",
    "#     try:\n",
    "#         assert(type(result)==type(expected_result))\n",
    "# #         print(result.shape)\n",
    "# #         print(expected_result.shape)\n",
    "#         assert(result.shape==expected_result.shape)\n",
    "#         assert(np.allclose(result,expected_result))\n",
    "\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         traceback.print_exc()\n",
    "#         return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
