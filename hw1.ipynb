{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Follow the instructions provided in the writeup to completely\n",
    "implement the class specifications for a basic MLP, optimizer, .\n",
    "You will be able to test each section individually by submitting\n",
    "to autolab after implementing what is required for that section\n",
    "-- do not worry if some methods required are not implemented yet.\n",
    "\n",
    "Notes:\n",
    "\n",
    "The __call__ method is a special reserved method in\n",
    "python that defines the behaviour of an object when it is\n",
    "used as a function. For example, take the Linear activation\n",
    "function whose implementation has been provided.\n",
    "\n",
    "# >>> activation = Identity()\n",
    "# >>> activation(3)\n",
    "# 3\n",
    "# >>> activation.forward(3)\n",
    "# 3\n",
    "\"\"\"\n",
    "\n",
    "# Do not import any additional 3rd party external libraries as they will not\n",
    "# be available to AutoLab and are not needed (or allowed)\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class Activation(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Interface for activation functions (non-linearities).\n",
    "\n",
    "    In all implementations, the state attribute must contain the result, i.e. the output of forward (it will be tested).\n",
    "    \"\"\"\n",
    "\n",
    "    # No additional work is needed for this class, as it acts like an abstract base class for the others\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def derivative(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Identity(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Identity function (already implemented).\n",
    "    \"\"\"\n",
    "\n",
    "    # This class is a gimme as it is already implemented for you as an example\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = x\n",
    "        return x\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Sigmoid non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    # Remember do not change the function signatures as those are needed to stay the same for AL\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Might we need to store something before returning?\n",
    "        sm = 1/(1+np.exp(-x))\n",
    "        self.state = sm\n",
    "\n",
    "        return sm\n",
    "\n",
    "    def derivative(self):\n",
    "\n",
    "        # Maybe something we need later in here...\n",
    "        sm_d = self.state * (1-self.state)\n",
    "\n",
    "        return sm_d\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Tanh non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    # This one's all you!\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        sm = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        self.state = sm\n",
    "\n",
    "        return sm\n",
    "\n",
    "    def derivative(self):\n",
    "        \n",
    "        return 1-self.state**2\n",
    "    \n",
    "    \n",
    "class ReLU(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    ReLU non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        a = np.copy(x)\n",
    "        a[a<0]=0\n",
    "        self.state = a\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        a = self.x\n",
    "        a[a>=0]=1\n",
    "        a[a<0]=0\n",
    "        return a\n",
    "\n",
    "# Ok now things get decidedly more interesting. The following Criterion class\n",
    "# will be used again as the basis for a number of loss functions (which are in the\n",
    "# form of classes so that they can be exchanged easily (it's how PyTorch and other\n",
    "# ML libraries do it))\n",
    "\n",
    "\n",
    "class Criterion(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Interface for loss functions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Nothing needs done to this class, it's used by the following Criterion classes\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logits = None\n",
    "        self.labels = None\n",
    "        self.loss = None\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return self.forward(x, y)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def derivative(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropy(Criterion):\n",
    "\n",
    "    \"\"\"\n",
    "    Softmax loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(SoftmaxCrossEntropy, self).__init__()\n",
    "        self.sm = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        self.logits = x.astype(float)\n",
    "        self.labels = y\n",
    "        self.loss = np.zeros(len(x)).astype(float)\n",
    "        \n",
    "        \n",
    "        #start my code\n",
    "        \n",
    "\n",
    "        for m in range(len(self.logits)):\n",
    "            a = np.max(self.logits[m]) \n",
    "            e = 0\n",
    "            for i in self.logits[m]:\n",
    "                e += np.exp(i-a)\n",
    "            a += np.log(e)\n",
    "            deno = np.exp(a)\n",
    "            aa = (np.exp(self.logits[m]))/deno\n",
    "            self.logits[m][:] = aa\n",
    "            loss = 0\n",
    "            for b in range(len(aa)):\n",
    "                if(self.labels[m][b]==1):\n",
    "                    loss -= np.log(aa[b])\n",
    "            self.loss[m] = loss\n",
    "        self.state = self.logits\n",
    "#         print(self.state)\n",
    "        return self.loss\n",
    "\n",
    "    def derivative(self):\n",
    "#         print(self.logits)\n",
    "\n",
    "        # self.sm might be useful here...\n",
    "\n",
    "        return self.logits-self.labels\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class BatchNorm(object):\n",
    "\n",
    "    def __init__(self, fan_in, alpha=0.9):\n",
    "\n",
    "        # You shouldn't need to edit anything in init\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.eps = 1e-8\n",
    "        self.x = None\n",
    "        self.norm = None\n",
    "        self.out = None\n",
    "\n",
    "        # The following attributes will be tested\n",
    "        self.var = np.ones((1, fan_in))\n",
    "        self.mean = np.zeros((1, fan_in))\n",
    "\n",
    "        self.gamma = np.ones((1, fan_in))\n",
    "        self.dgamma = np.zeros((1, fan_in))\n",
    "\n",
    "        self.beta = np.zeros((1, fan_in))\n",
    "        self.dbeta = np.zeros((1, fan_in))\n",
    "\n",
    "        # inference parameters\n",
    "        self.running_mean = np.zeros((1, fan_in))\n",
    "        self.running_var = np.ones((1, fan_in))\n",
    "\n",
    "    def __call__(self, x, eval=False):\n",
    "        return self.forward(x, eval)\n",
    "\n",
    "    def forward(self, x, eval=False):\n",
    "\n",
    "        # if eval:\n",
    "        #    # ???\n",
    "\n",
    "        self.x = x\n",
    "\n",
    "#         self.mean = \n",
    "        # self.var = # ???\n",
    "        # self.norm = # ???\n",
    "        # self.out = # ???\n",
    "\n",
    "        # update running batch statistics\n",
    "        # self.running_mean = # ???\n",
    "        # self.running_var = # ???\n",
    "\n",
    "        # ...\n",
    "\n",
    "        raise NotImplemented\n",
    "\n",
    "    def backward(self, delta):\n",
    "\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "# These are both easy one-liners, don't over-think them\n",
    "def random_normal_weight_init(d0, d1):\n",
    "    return  np.random.normal(size=(d0,d1))\n",
    "\n",
    "\n",
    "def zeros_bias_init(d):\n",
    "    return  np.random.normal(size=d)\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A simple multilayer perceptron\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, hiddens, activations, weight_init_fn, bias_init_fn, criterion, lr, momentum=0.0, num_bn_layers=0):\n",
    "\n",
    "        # Don't change this -->\n",
    "        self.train_mode = True\n",
    "        self.num_bn_layers = num_bn_layers\n",
    "        self.bn = num_bn_layers > 0\n",
    "        self.nlayers = len(hiddens) + 1\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activations = activations\n",
    "        self.criterion = criterion\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        # <---------------------\n",
    "\n",
    "        # Don't change the name of the following class attributes,\n",
    "        # the autograder will check against these attributes. But you will need to change\n",
    "        # the values in order to initialize them correctly\n",
    "\n",
    "        \n",
    "        \n",
    "        self.W = []\n",
    "        if len(hiddens) != 0 :\n",
    "            self.W = None\n",
    "            self.dW = None\n",
    "            self.b = None\n",
    "            self.db = None\n",
    "        else:\n",
    "            self.W = random_normal_weight_init(output_size,input_size)\n",
    "            self.dW = random_normal_weight_init(output_size,input_size)\n",
    "            self.b = zeros_bias_init(output_size)\n",
    "            self.db = zeros_bias_init(output_size)\n",
    "        # HINT: self.foo = [ bar(???) for ?? in ? ]\n",
    "\n",
    "        # if batch norm, add batch norm parameters\n",
    "        if self.bn:\n",
    "            self.bn_layers = None\n",
    "\n",
    "        # Feel free to add any other attributes useful to your implementation (input, output, ...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fwd = np.zeros((len(x),len(self.b)))\n",
    "        if self.nlayers == 1 :\n",
    "            count = 0\n",
    "            for i in x:\n",
    "                fwd[count] = (np.matmul(self.W ,i)+self.b) \n",
    "                count += 1\n",
    "            \n",
    "        \n",
    "        return fwd\n",
    "\n",
    "    def zero_grads(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def backward(self, labels):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.train_mode = False\n",
    "\n",
    "\n",
    "def get_training_stats(mlp, dset, nepochs, batch_size):\n",
    "\n",
    "    train, val, test = dset\n",
    "    trainx, trainy = train\n",
    "    valx, valy = val\n",
    "    testx, testy = test\n",
    "\n",
    "    idxs = np.arange(len(trainx))\n",
    "\n",
    "    training_losses = []\n",
    "    training_errors = []\n",
    "    validation_losses = []\n",
    "    validation_errors = []\n",
    "\n",
    "    # Setup ...\n",
    "\n",
    "    for e in range(nepochs):\n",
    "\n",
    "        # Per epoch setup ...\n",
    "\n",
    "        for b in range(0, len(trainx), batch_size):\n",
    "\n",
    "            pass  # Remove this line when you start implementing this\n",
    "            # Train ...\n",
    "\n",
    "        for b in range(0, len(valx), batch_size):\n",
    "\n",
    "            pass  # Remove this line when you start implementing this\n",
    "            # Val ...\n",
    "\n",
    "        # Accumulate data...\n",
    "\n",
    "    # Cleanup ...\n",
    "\n",
    "    for b in range(0, len(testx), batch_size):\n",
    "\n",
    "        pass  # Remove this line when you start implementing this\n",
    "        # Test ...\n",
    "\n",
    "    # Return results ...\n",
    "\n",
    "    # return (training_losses, training_errors, validation_losses, validation_errors)\n",
    "\n",
    "    raise NotImplemented\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = np.array([[1,1,1],[2,1,3]])\n",
    "t2 = np.array([[0,1,1],[1,1,1],[1,0,0],[0,1,0]])\n",
    "b = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(t1[0],t2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5 4 5]\n",
      "[5 8 5 5]\n"
     ]
    }
   ],
   "source": [
    "for i in t1:\n",
    "    print(np.matmul(t2,i)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(3, 4,[], Identity(),None, None, SoftmaxCrossEntropy(), 0.008, momentum=0.9, num_bn_layers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.23894839  1.05466295  1.14479539]\n",
      " [ 1.27796734  1.0526529   0.5240967 ]\n",
      " [ 0.45230216  1.93907953 -0.07985998]\n",
      " [ 0.66221135 -0.14925807 -1.04239598]]\n",
      "[-0.34483283 -1.4041975   0.45286166  0.45420925]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.61567712,  1.45051943,  2.76438337, -0.07523345],\n",
       "       [ 3.66631951,  3.77668016,  3.05696556, -1.49781406]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.forward(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A simple multilayer perceptron\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, hiddens, activations, weight_init_fn, bias_init_fn, criterion, lr, momentum=0.0, num_bn_layers=0):\n",
    "\n",
    "        # Don't change this -->\n",
    "        self.train_mode = True\n",
    "        self.num_bn_layers = num_bn_layers\n",
    "        self.bn = num_bn_layers > 0\n",
    "        self.nlayers = len(hiddens) + 1\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activations = activations\n",
    "        self.criterion = criterion\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        # <---------------------\n",
    "\n",
    "        # Don't change the name of the following class attributes,\n",
    "        # the autograder will check against these attributes. But you will need to change\n",
    "        # the values in order to initialize them correctly\n",
    "\n",
    "        \n",
    "        \n",
    "        self.W = []\n",
    "        self.W = random_normal_weight_init(output_size,input_size)\n",
    "        self.dW = random_normal_weight_init(output_size,input_size)\n",
    "        self.b = zeros_bias_init(output_size)\n",
    "        self.db = zeros_bias_init(output_size)\n",
    "#         self.W = np.zeros((output_size,input_size))\n",
    "#         self.dW = np.zeros((output_size,input_size))\n",
    "#         self.b = np.zeros(output_size)\n",
    "#         self.db = np.zeros(output_size)\n",
    "        # HINT: self.foo = [ bar(???) for ?? in ? ]\n",
    "\n",
    "        # if batch norm, add batch norm parameters\n",
    "        if self.bn:\n",
    "            self.bn_layers = None\n",
    "\n",
    "        # Feel free to add any other attributes useful to your implementation (input, output, ...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.W)\n",
    "        print(self.b)\n",
    "        fwd = np.zeros((len(x),len(self.b)))\n",
    "        if self.nlayers == 1 :\n",
    "            count = 0\n",
    "            for i in x:\n",
    "                fwd[count] = (np.matmul(self.W ,i)+self.b) \n",
    "                count += 1  \n",
    "        return fwd\n",
    "\n",
    "    def zero_grads(self):\n",
    "        self.dW = np.zeros((output_size,input_size))\n",
    "        self.db = np.zeros(output_size)\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def backward(self, labels):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.train_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44089364 -2.04987232  1.55329713]\n",
      " [-0.26374234  0.81856252 -1.70406089]\n",
      " [ 0.36002699 -0.60950001 -0.57045959]\n",
      " [ 0.9279269  -1.14135567  0.47059242]]\n",
      "[-0.92877499  1.38675834 -0.84459488 -0.56107637]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.86624381,  0.23751764, -1.6645275 , -0.30391272],\n",
       "       [ 0.79945681, -3.43434648, -2.44541969,  1.565199  ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = np.array([[1,1,1],[2,1,3]])\n",
    "t2 = np.array([[0,1,0],[1,0,0],[1,0,0],[0,1,0]])\n",
    "b = np.array([1,2,3,4])\n",
    "\n",
    "mlp = MLP(3, 4,[], Identity(),None, None, SoftmaxCrossEntropy(), 0.008, momentum=0.9, num_bn_layers=0)\n",
    "\n",
    "mlp.forward(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.09861229, 0.40760596])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = np.array([[1,1,1],[2,1,3]])\n",
    "t2 = np.array([[0,0,1],[0,0,1]])\n",
    "\n",
    "\n",
    "sme = SoftmaxCrossEntropy()\n",
    "sme.forward(t1,t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
